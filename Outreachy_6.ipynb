{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from pywikibot.data import api\n",
    "import pywikibot\n",
    "import pprint\n",
    "import re \n",
    "import requests\n",
    "\n",
    "from pywikibot import pagegenerators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_url(qurl):\n",
    "\n",
    "    request = requests.get(\"https://www.theguardian.com/\" + qurl)\n",
    "    if request.status_code == 200:\n",
    "        return qurl\n",
    "    else:\n",
    "        return None\n",
    "        \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qid_claim(page, text):\n",
    "\n",
    "    \n",
    "    qid_check=re.findall(r'[*]+\\s*[{{]+\\s*[G]+[u]+[a]+[r]+[d]+[i]+[a]+[n]+\\s*[t]+[o]+[p]+[i]+[c]+\\s*[|]+[^\\n]*',text)\n",
    "    \n",
    "    qid_page=[]  \n",
    "    try:\n",
    "        \n",
    "        for x in qid_check:\n",
    "            qid_check=x.split(\"|\")[1].replace(\"}\",\"\")\n",
    "            \n",
    "            qid_page.append(check_url(qid_check))   \n",
    "\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return qid_page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_claim(item, prop):\n",
    "    \"\"\"\n",
    "    Requires a property, item, and list of qualifier properties and items.\n",
    "    Returns claim that matches or None.\n",
    "    \"\"\"\n",
    "    \n",
    "    item_dict = item.get()\n",
    "    \n",
    "    claim_s=[]\n",
    "    claims_url=[]\n",
    "    \n",
    "    try:\n",
    "        item_dict['claims'][prop]\n",
    "\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "    for claim in item_dict['claims'][prop]:\n",
    "        if not claim.getSnakType() == 'somevalue':\n",
    "            #print(type(claim))\n",
    "            claim_s.append(claim)\n",
    "            print(claim.getTarget())\n",
    "            claims_url.append(claim.getTarget())\n",
    "        else:\n",
    "            pass   \n",
    "    return claim_s, claims_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addclaim(query,cat, qid_page):\n",
    "\n",
    "\n",
    "    \n",
    "    site = pywikibot.Site(\"en\", \"wikipedia\")\n",
    "    repo = site.data_repository()\n",
    "\n",
    "#Firstly I will make a page object using the title , then using the data_item() method I will connect to the wikidata item and edit\n",
    "    \n",
    "    for title_value in range(len(query)):\n",
    "        page = pywikibot.Page(site, query[title_value])\n",
    "        item=page.data_item()\n",
    "        \n",
    "        print(\"Title of page:\", query[title_value])\n",
    "        \n",
    "        target_id=qid_page[query[title_value]]\n",
    "        \n",
    "        print(\"Url against Wikipedia page with Guardian Topic Id, None represents invalid url encountered\", target_id,'\\n')\n",
    "        \n",
    "        print(\"Existing claim ID\")\n",
    "        \n",
    "        pre_claims_url=[]\n",
    "        \n",
    "        try:\n",
    "            pre_claim, pre_claims_url=get_claim(item,cat)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        diff=set(target_id)-set(pre_claims_url)\n",
    "        \n",
    "        print(\"Url to be added\", diff,'\\n')\n",
    "        \n",
    "        if len(diff)!=0:\n",
    "            for tar in diff:\n",
    "                if tar!=None:\n",
    "                    Identifiersclaim = pywikibot.Claim(repo, cat)\n",
    "                    Identifiersclaim.setTarget(tar)\n",
    "                    item.addClaim(Identifiersclaim, summary=u'Adding Guardian ID') \n",
    "\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "    \n",
    "    return None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cate(category):\n",
    "    \n",
    "    site = pywikibot.Site('en', 'wikipedia')\n",
    "    repo = site.data_repository() \n",
    "    #Creating an object of the Category class of the category to use the methods defined on the class\n",
    "\n",
    "    cat = pywikibot.Category(site, category)\n",
    "    property_value_cat=cat.templatesWithParams()[0][1][0].split(\"=\")[1]\n",
    "    #This is the property that is to be checked if present or not, else to be added to all the pages. \n",
    "\n",
    "    print(\"Property Value:\",property_value_cat)\n",
    "    \n",
    "    #To generate an iterator for the pages in the category\n",
    "\n",
    "    gen = pywikibot.pagegenerators.CategorizedPageGenerator(cat)\n",
    "    \n",
    "    query=[]\n",
    "    \n",
    "    for page in gen:\n",
    "        query.append(page.title())\n",
    "    \n",
    "    #print(\"Pages inside the category\", query)\n",
    "    \n",
    "    #This list will consist of all the url links which are to be set as target for the ID claim\n",
    "    qid_page={}\n",
    "\n",
    "    for page_title in query:\n",
    "        page = pywikibot.Page(site, page_title)\n",
    "        text=page.text\n",
    "        qid_page[page_title]=qid_claim(page, text)\n",
    "        \n",
    "    #print(qid_page)\n",
    "    \n",
    "    addclaim(query,property_value_cat, qid_page)\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#references are not shown in page.text and hence some times it is not working "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Property Value: P3106\n",
      "Title of page: Arab Spring\n",
      "Url against Wikipedia page with Guardian Topic Id, None represents invalid url encountered ['world/arab-and-middle-east-protests', 'world/interactive/2011/mar/22/middle-east-protest-interactive-timeline'] \n",
      "\n",
      "Existing claim ID\n",
      "world/arab-and-middle-east-protests\n",
      "Url to be added {'world/interactive/2011/mar/22/middle-east-protest-interactive-timeline'} \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sleeping for 9.6 seconds, 2021-05-06 18:28:16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title of page: Jimi Hendrix\n",
      "Url against Wikipedia page with Guardian Topic Id, None represents invalid url encountered [None] \n",
      "\n",
      "Existing claim ID\n",
      "music/jimi-hendrix\n",
      "Url to be added {None} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#https://bitbucket.org/mikepeel/wikicode/src/master/enwp_find_wikidata.py\n",
    "\n",
    "category=\"Guardian topic ID different from Wikidata\"\n",
    "\n",
    "cate(category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Property Value: P3106\n",
      "Title of page: Arab Spring\n",
      "Url against Wikipedia page with Guardian Topic Id, None represents invalid url encountered ['world/arab-and-middle-east-protests', 'world/interactive/2011/mar/22/middle-east-protest-interactive-timeline'] \n",
      "\n",
      "Existing claim ID\n",
      "world/arab-and-middle-east-protests\n",
      "world/interactive/2011/mar/22/middle-east-protest-interactive-timeline\n",
      "Url to be added set() \n",
      "\n",
      "Title of page: Jimi Hendrix\n",
      "Url against Wikipedia page with Guardian Topic Id, None represents invalid url encountered [None] \n",
      "\n",
      "Existing claim ID\n",
      "music/jimi-hendrix\n",
      "Url to be added {None} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Re-run to check the changes \n",
    "\n",
    "category=\"Guardian topic ID different from Wikidata\"\n",
    "\n",
    "cate(category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Property Value: P3106\n",
      "Title of page: Arab Spring concurrent incidents\n",
      "Url against Wikipedia page with Guardian Topic Id, None represents invalid url encountered ['world/arab-and-middle-east-protests', 'world/interactive/2011/mar/22/middle-east-protest-interactive-timeline'] \n",
      "\n",
      "Existing claim ID\n",
      "Url to be added {'world/interactive/2011/mar/22/middle-east-protest-interactive-timeline', 'world/arab-and-middle-east-protests'} \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sleeping for 19.5 seconds, 2021-05-06 18:37:59\n"
     ]
    }
   ],
   "source": [
    "category=\"Guardian topic ID not in Wikidata\"\n",
    "\n",
    "cate(category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
